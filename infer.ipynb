{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45edcb91-6713-40b8-819b-12415cc56449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}\n",
      "('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')\n"
     ]
    }
   ],
   "source": [
    "#@author yuetian\n",
    "from torchvision.ops.boxes import batched_nms\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from models.prob_deformable_detr import build\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from main_open_world import get_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets.samplers as samplers\n",
    "from torchvision.transforms import functional as F\n",
    "from util.misc import init_distributed_mode, collate_fn, MetricLogger, SmoothedValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733c61d-e40d-4689-be29-403968b96ce8",
   "metadata": {},
   "source": [
    "# Reference: params setting for task 1 eval\n",
    "```\n",
    "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=5, weight_decay=0.0001, epochs=191, lr_drop=35, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='exps/SOWODB/PROB/eval', device='cuda', seed=42, resume='', start_epoch=0, eval=True, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=0, CUR_INTRODUCED_CLS=19, unmatched_boxes=False, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=False, bbox_thresh=0.3, pretrain='exps/SOWODB/PROB/t1.pth', nc_loss_coef=2, train_set='owdetr_t1_train', test_set='owdetr_test', num_classes=81, nc_epoch=0, dataset='OWDETR', data_root='/training_data_2/yuetian/OWOD', unk_conf_w=1.0, model_type='prob', wandb_name='prob_original', wandb_project='', obj_loss_coef=0.0008, obj_temp=1.3, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1530a3a5-d9a3-4423-8145-10fa775523ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'lr': 0.0002, 'lr_backbone_names': ['backbone.0'], \n",
    "    'lr_backbone': 2e-05, 'lr_linear_proj_names': ['reference_points', 'sampling_offsets'], \n",
    "    'lr_linear_proj_mult': 0.1, 'batch_size': 1, 'weight_decay': 0.0001, 'epochs': 191, 'lr_drop': 35, \n",
    "    'lr_drop_epochs': None, 'clip_max_norm': 0.1, 'sgd': False, 'with_box_refine': False, 'two_stage': False, \n",
    "    'masks': False, 'backbone': 'dino_resnet50', 'frozen_weights': None, 'dilation': False, 'position_embedding': 'sine', \n",
    "    'position_embedding_scale': 6.283185307179586, 'num_feature_levels': 4, 'enc_layers': 6, 'dec_layers': 6, \n",
    "    'dim_feedforward': 1024, 'hidden_dim': 256, 'dropout': 0.1, 'nheads': 8, 'num_queries': 100, \n",
    "    'dec_n_points': 4, 'enc_n_points': 4, 'aux_loss': True, 'set_cost_class': 2, 'set_cost_bbox': 5, \n",
    "    'set_cost_giou': 2, 'cls_loss_coef': 2, 'bbox_loss_coef': 5, 'giou_loss_coef': 2, \n",
    "    'focal_alpha': 0.25, 'coco_panoptic_path': None, 'remove_difficult': False, 'output_dir': 'exps/SOWODB/PROB/eval', \n",
    "    'device': 'cuda:2', 'seed': 42, 'resume': '', 'start_epoch': 0, 'eval': True, 'viz': False, 'eval_every': 5, 'num_workers': 3, \n",
    "    'cache_mode': False, 'PREV_INTRODUCED_CLS': 0, 'CUR_INTRODUCED_CLS': 19, 'unmatched_boxes': False, 'top_unk': 5, 'featdim': 1024, \n",
    "    'invalid_cls_logits': False, 'NC_branch': False, 'bbox_thresh': 0.3, 'pretrain': 'exps/SOWODB/PROB/t1.pth', 'nc_loss_coef': 2, \n",
    "    'train_set': 'owdetr_t1_train', 'test_set': 'owdetr_test', 'num_classes': 81, 'nc_epoch': 0, 'dataset': 'OWDETR', \n",
    "    'data_root': '/training_data_2/yuetian/OWOD', 'unk_conf_w': 1.0, 'model_type': 'prob', 'wandb_name': 'prob_original', \n",
    "    'wandb_project': '', 'obj_loss_coef': 0.0008, 'obj_temp': 1.3, 'freeze_prob_model': False, 'num_inst_per_class': 50, \n",
    "    'exemplar_replay_selection': False, 'exemplar_replay_max_length': 10000000000.0, 'exemplar_replay_dir': '', \n",
    "    'exemplar_replay_prev_file': '', 'exemplar_replay_cur_file': '', 'exemplar_replay_random': False\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e1a897-13c1-4f01-acd4-81246ce79794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')\n"
     ]
    }
   ],
   "source": [
    "VOC_CLASS_NAMES_COCOFIED = [\n",
    "    \"airplane\",  \"dining table\", \"motorcycle\",\n",
    "    \"potted plant\", \"couch\", \"tv\"\n",
    "]\n",
    "\n",
    "BASE_VOC_CLASS_NAMES = [\n",
    "    \"aeroplane\", \"diningtable\", \"motorbike\",\n",
    "    \"pottedplant\",  \"sofa\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "VOC_CLASS_NAMES = [\n",
    "    \"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bus\",\"car\",\n",
    "    \"cat\",\"cow\",\"dog\",\"horse\",\"motorbike\",\"sheep\",\"train\",\n",
    "    \"elephant\",\"bear\",\"zebra\",\"giraffe\",\"truck\",\"person\"\n",
    "]\n",
    "\n",
    "T2_CLASS_NAMES = [\n",
    "    \"traffic light\",\"fire hydrant\",\"stop sign\",\n",
    "    \"parking meter\",\"bench\",\"chair\",\"diningtable\",\n",
    "    \"pottedplant\",\"backpack\",\"umbrella\",\"handbag\",\n",
    "    \"tie\",\"suitcase\",\"microwave\",\"oven\",\"toaster\",\"sink\",\n",
    "    \"refrigerator\",\"bed\",\"toilet\",\"sofa\"\n",
    "]\n",
    "\n",
    "T3_CLASS_NAMES = [\n",
    "    \"frisbee\",\"skis\",\"snowboard\",\"sports ball\",\n",
    "    \"kite\",\"baseball bat\",\"baseball glove\",\"skateboard\",\n",
    "    \"surfboard\",\"tennis racket\",\"banana\",\"apple\",\"sandwich\",\n",
    "    \"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\"donut\",\"cake\"\n",
    "]\n",
    "\n",
    "T4_CLASS_NAMES = [\n",
    "    \"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cell phone\",\"book\",\n",
    "    \"clock\",\"vase\",\"scissors\",\"teddy bear\",\"hair drier\",\"toothbrush\",\n",
    "    \"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"tvmonitor\",\"bottle\"\n",
    "]\n",
    "\n",
    "UNK_CLASS = [\"unknown\"]\n",
    "\n",
    "VOC_COCO_CLASS_NAMES = tuple(itertools.chain(VOC_CLASS_NAMES, T2_CLASS_NAMES, T3_CLASS_NAMES, T4_CLASS_NAMES, UNK_CLASS))\n",
    "print(VOC_COCO_CLASS_NAMES)\n",
    "\n",
    "CLASSES = list(VOC_COCO_CLASS_NAMES)\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8bfacfb-c10a-4f22-8252-b262991fea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(model, criterion, data_loader, dataset, device, output_dir='./output', allow_unknown=True):\n",
    "    mode = \"no_unknown\" if not allow_unknown else \"filtered_unknown\"\n",
    "    os.makedirs(os.path.join(output_dir, f\"./vali_loader/{mode}/\"), exist_ok=True)\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "\n",
    "    use_topk = True\n",
    "    num_obj = 20\n",
    "    for batch_idx, (samples, targets) in enumerate(tqdm(data_loader)):\n",
    "        if batch_idx >=10:\n",
    "            break\n",
    "        print(samples[0].shape)\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        top_k = len(targets[0]['boxes'])\n",
    "\n",
    "        outputs = model(samples)\n",
    "        if allow_unknown:\n",
    "            probas = outputs['pred_logits'].softmax(-1)[0, :, :].cpu()\n",
    "        else:\n",
    "            probas = outputs['pred_logits'].softmax(-1)[0, :, :-1].cpu()\n",
    "        pred_objs = outputs['pred_obj'].softmax(-1)[0, :].cpu()\n",
    "        predicted_boxes = outputs['pred_boxes'][0,].cpu()\n",
    "        scores, predicted_boxes = filter_boxes(probas, predicted_boxes)\n",
    "        labels = scores.argmax(axis=1)\n",
    "        scores = scores.max(-1).values\n",
    "        \n",
    "    # Filter if contains unknown\n",
    "    if allow_unknown:\n",
    "        print(\"start_filtering\")\n",
    "        iou_threshold = 0.4\n",
    "        unknown_class_idx = 80  # Assuming the last class is \"unknown\"\n",
    "        keep_indices = []\n",
    "\n",
    "        # Get indices of \"unknown\" predictions\n",
    "        unknown_indices = (labels == unknown_class_idx).nonzero(as_tuple=True)[0]\n",
    "        # print(f\"unknown indicies: {unknown_indices} - {labels}\")\n",
    "        \n",
    "        # Loop over each \"unknown\" bounding box\n",
    "        for idx, (box, lbl) in enumerate(zip(predicted_boxes, labels)):\n",
    "            if lbl == unknown_class_idx:\n",
    "                keep_unknown = True\n",
    "                for known_box in predicted_boxes[labels != unknown_class_idx]:\n",
    "                    if overlap_rate(image_tensor[0:1], box, known_box) > iou_threshold:\n",
    "                        keep_unknown = False\n",
    "                        # print(\"drop\")\n",
    "                        break\n",
    "                if keep_unknown:\n",
    "                    keep_indices.append(idx)\n",
    "            else:\n",
    "                keep_indices.append(idx)\n",
    "                \n",
    "        # Use the indices to filter the predictions\n",
    "        predicted_boxes = predicted_boxes[keep_indices]\n",
    "        labels = labels[keep_indices]\n",
    "        scores = scores[keep_indices]\n",
    "\n",
    "        # Save bbox, label, and probability info\n",
    "        save_bbox_info(os.path.join(output_dir, f\"./vali_loader/{mode}/\"), \n",
    "                       targets[0][\"image_id\"][0], \n",
    "                       predicted_boxes, \n",
    "                       labels, \n",
    "                       scores,\n",
    "                       samples.tensors[0:1])\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(10,3), dpi=200)\n",
    "\n",
    "        # Ori Picture\n",
    "        plot_ori_image(\n",
    "            samples.tensors[0:1],\n",
    "            ax[0], \n",
    "            plot_prob=False,\n",
    "        )\n",
    "        ax[0].set_title('Original Image')\n",
    "\n",
    "        # Pred results\n",
    "        # if not control the number of labels\n",
    "        if not use_topk:\n",
    "            plot_prediction(\n",
    "                samples.tensors[0:1], \n",
    "                scores[-num_obj:], \n",
    "                predicted_boxes[-num_obj:].detach().cpu(), \n",
    "                labels[-num_obj:], \n",
    "                ax[1], \n",
    "                plot_prob=False,\n",
    "                dataset=dataset,\n",
    "            )\n",
    "        # if control the number of labels\n",
    "        if use_topk:\n",
    "            plot_prediction(\n",
    "                samples.tensors[0:1], \n",
    "                scores[-top_k:], \n",
    "                predicted_boxes[-top_k:].detach().cpu(), \n",
    "                labels[-top_k:], \n",
    "                ax[1], \n",
    "                plot_prob=False,\n",
    "                dataset=dataset,\n",
    "            )\n",
    "        ax[1].set_title('Prediction (Ours)')\n",
    "\n",
    "        # GT Results\n",
    "        plot_prediction(\n",
    "            samples.tensors[0:1], \n",
    "            torch.ones(targets[0]['boxes'].shape[0]), \n",
    "            targets[0]['boxes'].detach().cpu(), \n",
    "            targets[0]['labels'],\n",
    "            ax[2], \n",
    "            plot_prob=False,\n",
    "            dataset=dataset,\n",
    "        )\n",
    "        ax[2].set_title('GT')\n",
    "\n",
    "        for i in range(3):\n",
    "            ax[i].set_aspect('equal')\n",
    "            ax[i].set_axis_off()\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, f'vali_loader/{mode}/img_{int(targets[0][\"image_id\"][0])}.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a29eea5-7a9f-4044-8da7-97b3af4fb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_single_img(model, criterion, image_path, dataset, device, output_dir='./output', allow_unknown=True):\n",
    "    mode = \"no_unknown\" if not allow_unknown else \"filtered_unknown\"\n",
    "    os.makedirs(os.path.join(output_dir, f\"./custom/{mode}/\"), exist_ok=True)\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    # Load the image and preprocess\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    # Normalize the image tensor\n",
    "    means = torch.tensor([0.485, 0.456, 0.406]).to(device).view(1, 3, 1, 1)\n",
    "    stds = torch.tensor([0.229, 0.224, 0.225]).to(device).view(1, 3, 1, 1)\n",
    "    image_tensor = (image_tensor - means) / stds\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(image_tensor)\n",
    "    if allow_unknown:\n",
    "        probas = outputs['pred_logits'].softmax(-1)[0, :, :].cpu()\n",
    "    else:\n",
    "        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1].cpu()\n",
    "    predicted_boxes = outputs['pred_boxes'][0,].cpu()\n",
    "    scores, predicted_boxes = filter_boxes(probas, predicted_boxes)\n",
    "    labels = scores.argmax(axis=1)\n",
    "    scores = scores.max(-1).values\n",
    "    \n",
    "    # Filter if contains unknown\n",
    "    if allow_unknown:\n",
    "        print(\"start_filtering\")\n",
    "        iou_threshold = 0.4\n",
    "        unknown_class_idx = 80  # Assuming the last class is \"unknown\"\n",
    "        keep_indices = []\n",
    "\n",
    "        # Get indices of \"unknown\" predictions\n",
    "        unknown_indices = (labels == unknown_class_idx).nonzero(as_tuple=True)[0]\n",
    "        # print(f\"unknown indicies: {unknown_indices} - {labels}\")\n",
    "        \n",
    "        # Loop over each \"unknown\" bounding box\n",
    "        for idx, (box, lbl) in enumerate(zip(predicted_boxes, labels)):\n",
    "            if lbl == unknown_class_idx:\n",
    "                keep_unknown = True\n",
    "                for known_box in predicted_boxes[labels != unknown_class_idx]:\n",
    "                    if overlap_rate(image_tensor[0:1], box, known_box) > iou_threshold:\n",
    "                        keep_unknown = False\n",
    "                        # print(\"drop\")\n",
    "                        break\n",
    "                if keep_unknown:\n",
    "                    keep_indices.append(idx)\n",
    "            else:\n",
    "                keep_indices.append(idx)\n",
    "                \n",
    "        # Use the indices to filter the predictions\n",
    "        predicted_boxes = predicted_boxes[keep_indices]\n",
    "        labels = labels[keep_indices]\n",
    "        scores = scores[keep_indices]\n",
    "   \n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,3), dpi=200)\n",
    "\n",
    "    # Original Image\n",
    "    plot_ori_image(image_tensor[0:1], ax[0], plot_prob=False)\n",
    "    ax[0].set_title('Original Image')\n",
    "\n",
    "    # Prediction\n",
    "    top_k = len(predicted_boxes)\n",
    "    plot_prediction(\n",
    "        image_tensor[0:1], \n",
    "        scores[-top_k:], \n",
    "        predicted_boxes[-top_k:].detach().cpu(), \n",
    "        labels[-top_k:], \n",
    "        ax[1], \n",
    "        plot_prob=False,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    ax[1].set_title('Prediction (Ours)')\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].set_aspect('equal')\n",
    "        ax[i].set_axis_off()\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, f'viz_{os.path.basename(image_path)}'))\n",
    "    save_bbox_info(os.path.join(output_dir, f\"./custom/{mode}/\"), os.path.basename(image_path), predicted_boxes, labels, scores, [image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2771dd50-8f86-4e04-a9fd-dc0d2e652490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_rate(image, boxA, boxB):\n",
    "    h, w = image[0].permute(1, 2, 0).detach().cpu().numpy().shape[:-1]\n",
    "    boxA = rescale_bboxes(boxA, [w, h]).detach().cpu().numpy()\n",
    "    boxB = rescale_bboxes(boxB, [w, h]).detach().cpu().numpy()\n",
    "    # print(boxA, boxB)\n",
    "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # Compute the area of box A\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "\n",
    "    # Compute the overlap rate\n",
    "    rate = interArea / boxAArea\n",
    "    return rate\n",
    "\n",
    "### You can choose confidence: the default value of confidence is 0.7\n",
    "def filter_boxes(scores, boxes, confidence=0.7, apply_nms=True, iou=0.5):\n",
    "    keep = scores.max(-1).values > confidence\n",
    "    scores, boxes = scores[keep], boxes[keep]\n",
    " \n",
    "    if apply_nms:\n",
    "        top_scores, labels = scores.max(-1)\n",
    "        keep = batched_nms(boxes, top_scores, labels, iou)\n",
    "        scores, boxes = scores[keep], boxes[keep]\n",
    " \n",
    "    return scores, boxes\n",
    "\n",
    "def save_bbox_info(output_dir, image_id, bboxes, labels, scores, images):\n",
    "    \"\"\"\n",
    "    Save bounding box, label, and probability info to a text file.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(output_dir, f'info_{image_id}.txt'), 'w') as f:\n",
    "        for bbox, label_index, score, image in zip(bboxes, labels, scores, images):\n",
    "            boxes = [rescale_bboxes(boxes[i], [w, h]).cpu() for i in range(len(boxes))]\n",
    "            h, w = pil_img.shape[:-1]\n",
    "            label_name = CLASSES[label_index]\n",
    "            f.write(f\"BBOX: {bbox.tolist()}, LABEL: {label_name} (ID: {label_index.item()}), PROB: {score.item()}\\n\")\n",
    "\n",
    "def bbox_iou(boxA, boxB):\n",
    "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # print(iou)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def plot_image(ax, img, norm):\n",
    "    if norm:\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = (img * 255)\n",
    "    img = img.astype('uint8')\n",
    "    ax.imshow(img)\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "    \n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    # print(size)\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32).to(out_bbox)\n",
    "    return b\n",
    "\n",
    "def plot_prediction(image, scores, boxes, labels, ax=None, plot_prob=True, dataset='OWOD'):    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    plot_results(image[0].permute(1, 2, 0).detach().cpu().numpy(), scores, boxes, labels, ax, plot_prob=plot_prob, dataset=dataset)\n",
    "\n",
    "def plot_ori_image(image, ax=None, plot_prob=False):    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    plot_results(image[0].permute(1, 2, 0).detach().cpu().numpy(), None, None, None, ax)\n",
    "\n",
    "def plot_results(pil_img, scores, boxes, labels, ax, plot_prob=True, norm=True, dataset='OWOD'):\n",
    "    from matplotlib import pyplot as plt\n",
    "    h, w = pil_img.shape[:-1]\n",
    "    # w, h = pil_img.shape[:-1]\n",
    "    image = plot_image(ax, pil_img, norm)\n",
    "    colors = COLORS * 100\n",
    "    if boxes is not None:\n",
    "        boxes = [rescale_bboxes(boxes[i], [w, h]).cpu() for i in range(len(boxes))]\n",
    "        for sc, cl, (xmin, ymin, xmax, ymax), c in zip(scores, labels, boxes, colors):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                       fill=False, color=c, linewidth=2))\n",
    "            # print(dataset, cl)\n",
    "            # text = f'{CLASSES[str(dataset)][cl]}: {sc:0.2f}'\n",
    "            text = f'{CLASSES[cl]}: {sc:0.2f}'\n",
    "            ax.text(xmin, ymin, text, fontsize=5, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    ax.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c297634-519a-4c1b-b0af-44ea8e8ddb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid class range: [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "DINO resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheny63/anaconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cheny63/anaconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with exemplar_replay_selection\n",
      "Initialized from the pre-training model\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model, criterion, postprocessors, exemplar_selection = build(args)\n",
    "model.to(torch.device(args.device))\n",
    "\n",
    "print('Initialized from the pre-training model')\n",
    "checkpoint = torch.load(args.pretrain, map_location='cpu')\n",
    "state_dict = checkpoint['model']\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "args.start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2054ea90-7854-4a7b-b28a-3fe57479d561",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWDETR\n",
      "owdetr_t1_train\n",
      "owdetr_test\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 89490\n",
      "    Root location: /training_data_2/yuetian/OWOD\n",
      "    [['train'], Compose(\n",
      "    <datasets.transforms.RandomHorizontalFlip object at 0x7fbc34e977f0>\n",
      "    <datasets.transforms.RandomSelect object at 0x7fbc34e96d40>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7fbc34e97d90>\n",
      "    <datasets.transforms.Normalize object at 0x7fbc34e96d70>\n",
      ")\n",
      ")]\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 4952\n",
      "    Root location: /training_data_2/yuetian/OWOD\n",
      "    [['test'], Compose(\n",
      "    <datasets.transforms.RandomResize object at 0x7fbc34e97be0>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7fbc34e979a0>\n",
      "    <datasets.transforms.Normalize object at 0x7fbc34e97940>\n",
      ")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_val = get_datasets(args)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=collate_fn, num_workers=args.num_workers,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c400ba-86bc-4d31-a7a4-f628c37df201",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/4952 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NestedTensor' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mviz\u001b[0;34m(model, criterion, data_loader, dataset, device, output_dir, allow_unknown)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     16\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NestedTensor' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "viz(model, criterion, data_loader_val, args.dataset, args.device, allow_unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "675353b3-fb8f-48e3-bbbf-99458c9eb2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1086, 1926])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(\"samples/horse.jpg\")\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0)\n",
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1562e15a-a3c2-42e2-b167-53b4463f174b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Not implemented on the CPU",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mviz_single_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msamples/000000289343.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mviz_single_img\u001b[0;34m(model, criterion, image_path, dataset, device, output_dir, allow_unknown)\u001b[0m\n\u001b[1;32m     13\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m (image_tensor \u001b[38;5;241m-\u001b[39m means) \u001b[38;5;241m/\u001b[39m stds\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_unknown:\n\u001b[1;32m     18\u001b[0m     probas \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_logits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, :, :]\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/anaconda3/envs/prob/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/experiment/PROB/models/prob_deformable_detr.py:247\u001b[0m, in \u001b[0;36mDeformableDETR.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtwo_stage:\n\u001b[1;32m    246\u001b[0m     query_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_embed\u001b[38;5;241m.\u001b[39mweight\n\u001b[0;32m--> 247\u001b[0m hs, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m outputs_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    250\u001b[0m outputs_coords \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/prob/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/experiment/PROB/models/deformable_transformer.py:159\u001b[0m, in \u001b[0;36mDeformableTransformer.forward\u001b[0;34m(self, srcs, masks, pos_embeds, query_embed)\u001b[0m\n\u001b[1;32m    156\u001b[0m valid_ratios \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_valid_ratio(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m masks], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# encoder\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_flatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvl_pos_embed_flatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_flatten\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# prepare input for decoder\u001b[39;00m\n\u001b[1;32m    162\u001b[0m bs, _, c \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/prob/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/experiment/PROB/models/deformable_transformer.py:264\u001b[0m, in \u001b[0;36mDeformableTransformerEncoder.forward\u001b[0;34m(self, src, spatial_shapes, level_start_index, valid_ratios, pos, padding_mask)\u001b[0m\n\u001b[1;32m    262\u001b[0m reference_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_reference_points(spatial_shapes, valid_ratios, device\u001b[38;5;241m=\u001b[39msrc\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 264\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/prob/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/experiment/PROB/models/deformable_transformer.py:229\u001b[0m, in \u001b[0;36mDeformableTransformerEncoderLayer.forward\u001b[0;34m(self, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, pos, reference_points, spatial_shapes, level_start_index, padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# self attention\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     src2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_pos_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(src2)\n\u001b[1;32m    231\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(src)\n",
      "File \u001b[0;32m~/anaconda3/envs/prob/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/experiment/PROB/models/ops/modules/ms_deform_attn.py:112\u001b[0m, in \u001b[0;36mMSDeformAttn.forward\u001b[0;34m(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast dim of reference_points must be 2 or 4, but get \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reference_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mMSDeformAttnFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_spatial_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_level_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim2col_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(output)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/experiment/PROB/models/ops/functions/ms_deform_attn_func.py:25\u001b[0m, in \u001b[0;36mMSDeformAttnFunction.forward\u001b[0;34m(ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step):\n\u001b[1;32m     24\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mim2col_step \u001b[38;5;241m=\u001b[39m im2col_step\n\u001b[0;32m---> 25\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mMSDA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_deform_attn_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_spatial_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_level_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim2col_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Not implemented on the CPU"
     ]
    }
   ],
   "source": [
    "viz_single_img(model, criterion, \"samples/000000289343.jpg\", args.dataset, args.device, allow_unknown=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa304108-1f66-4552-b31f-2d511eb6855f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob",
   "language": "python",
   "name": "prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
