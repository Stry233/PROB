{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45edcb91-6713-40b8-819b-12415cc56449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}\n",
      "('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')\n"
     ]
    }
   ],
   "source": [
    "#@author yuetian\n",
    "from torchvision.ops.boxes import batched_nms\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from models.prob_deformable_detr import build\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from main_open_world import get_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets.samplers as samplers\n",
    "from torchvision.transforms import functional as F\n",
    "from util.misc import init_distributed_mode, collate_fn, MetricLogger, SmoothedValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733c61d-e40d-4689-be29-403968b96ce8",
   "metadata": {},
   "source": [
    "# Config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1530a3a5-d9a3-4423-8145-10fa775523ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = 'exps/SOWODB/PROB/t1.pth'\n",
    "case = \"SOWODB\" # \"MOWODB\"\n",
    "\n",
    "# SOWODB Case\n",
    "args_dicts = {\"SOWODB\": {\n",
    "    'lr': 0.0002, 'lr_backbone_names': ['backbone.0'], \n",
    "    'lr_backbone': 2e-05, 'lr_linear_proj_names': ['reference_points', 'sampling_offsets'], \n",
    "    'lr_linear_proj_mult': 0.1, 'batch_size': 1, 'weight_decay': 0.0001, 'epochs': 191, 'lr_drop': 35, \n",
    "    'lr_drop_epochs': None, 'clip_max_norm': 0.1, 'sgd': False, 'with_box_refine': False, 'two_stage': False, \n",
    "    'masks': False, 'backbone': 'dino_resnet50', 'frozen_weights': None, 'dilation': False, 'position_embedding': 'sine', \n",
    "    'position_embedding_scale': 6.283185307179586, 'num_feature_levels': 4, 'enc_layers': 6, 'dec_layers': 6, \n",
    "    'dim_feedforward': 1024, 'hidden_dim': 256, 'dropout': 0.1, 'nheads': 8, 'num_queries': 100, \n",
    "    'dec_n_points': 4, 'enc_n_points': 4, 'aux_loss': True, 'set_cost_class': 2, 'set_cost_bbox': 5, \n",
    "    'set_cost_giou': 2, 'cls_loss_coef': 2, 'bbox_loss_coef': 5, 'giou_loss_coef': 2, \n",
    "    'focal_alpha': 0.25, 'coco_panoptic_path': None, 'remove_difficult': False, 'output_dir': '', \n",
    "    'device': 'cuda:0', 'seed': 42, 'resume': '', 'start_epoch': 0, 'eval': True, 'viz': False, 'eval_every': 5, 'num_workers': 3, \n",
    "    'cache_mode': False, 'PREV_INTRODUCED_CLS': 0, 'CUR_INTRODUCED_CLS': 19, 'unmatched_boxes': False, 'top_unk': 5, 'featdim': 1024, \n",
    "    'invalid_cls_logits': False, 'NC_branch': False, 'bbox_thresh': 0.3, 'pretrain': pretrain, 'nc_loss_coef': 2, \n",
    "    'train_set': 'owdetr_t1_train', 'test_set': 'owdetr_test', 'num_classes': 81, 'nc_epoch': 0, 'dataset': 'OWDETR', \n",
    "    'data_root': '/training_data_2/yuetian/OWOD', 'unk_conf_w': 1.0, 'model_type': 'prob', 'wandb_name': 'prob_original', \n",
    "    'wandb_project': '', 'obj_loss_coef': 0.0008, 'obj_temp': 1.3, 'freeze_prob_model': False, 'num_inst_per_class': 50, \n",
    "    'exemplar_replay_selection': False, 'exemplar_replay_max_length': 10000000000.0, 'exemplar_replay_dir': '', \n",
    "    'exemplar_replay_prev_file': '', 'exemplar_replay_cur_file': '', 'exemplar_replay_random': False\n",
    "}, \"MOWODB\":\n",
    "    {\n",
    "    'lr': 0.0002, 'lr_backbone_names': ['backbone.0'], \n",
    "    'lr_backbone': 2e-05, 'lr_linear_proj_names': ['reference_points', 'sampling_offsets'], \n",
    "    'lr_linear_proj_mult': 0.1, 'batch_size': 1, 'weight_decay': 0.0001, 'epochs': 191, 'lr_drop': 35, \n",
    "    'lr_drop_epochs': None, 'clip_max_norm': 0.1, 'sgd': False, 'with_box_refine': False, 'two_stage': False, \n",
    "    'masks': False, 'backbone': 'dino_resnet50', 'frozen_weights': None, 'dilation': False, 'position_embedding': 'sine', \n",
    "    'position_embedding_scale': 6.283185307179586, 'num_feature_levels': 4, 'enc_layers': 6, 'dec_layers': 6, \n",
    "    'dim_feedforward': 1024, 'hidden_dim': 256, 'dropout': 0.1, 'nheads': 8, 'num_queries': 100, \n",
    "    'dec_n_points': 4, 'enc_n_points': 4, 'aux_loss': True, 'set_cost_class': 2, 'set_cost_bbox': 5, \n",
    "    'set_cost_giou': 2, 'cls_loss_coef': 2, 'bbox_loss_coef': 5, 'giou_loss_coef': 2, \n",
    "    'focal_alpha': 0.25, 'coco_panoptic_path': None, 'remove_difficult': False, 'output_dir': '', \n",
    "    'device': 'cuda:0', 'seed': 42, 'resume': '', 'start_epoch': 0, 'eval': True, 'viz': False, 'eval_every': 5, 'num_workers': 3, \n",
    "    'cache_mode': False, 'PREV_INTRODUCED_CLS': 0, 'CUR_INTRODUCED_CLS': 19, 'unmatched_boxes': False, 'top_unk': 5, 'featdim': 1024, \n",
    "    'invalid_cls_logits': False, 'NC_branch': False, 'bbox_thresh': 0.3, 'pretrain': pretrain, 'nc_loss_coef': 2, \n",
    "    'train_set': 'owod_t1_train', 'test_set': 'owod_all_task_test', 'num_classes': 81, 'nc_epoch': 0, 'dataset': 'TOWOD', \n",
    "    'data_root': '/training_data_2/yuetian/OWOD', 'unk_conf_w': 1.0, 'model_type': 'prob', 'wandb_name': 'prob_original', \n",
    "    'wandb_project': '', 'obj_loss_coef': 0.0008, 'obj_temp': 1.3, 'freeze_prob_model': False, 'num_inst_per_class': 50, \n",
    "    'exemplar_replay_selection': False, 'exemplar_replay_max_length': 10000000000.0, 'exemplar_replay_dir': '', \n",
    "    'exemplar_replay_prev_file': '', 'exemplar_replay_cur_file': '', 'exemplar_replay_random': False\n",
    "}}\n",
    "\n",
    "args = argparse.Namespace(**args_dicts[case])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e1a897-13c1-4f01-acd4-81246ce79794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "class_name = {'OWDETR': (\n",
    "'aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train',\n",
    "'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
    "'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard',\n",
    "'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple',\n",
    "'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote',\n",
    "'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass',\n",
    "'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), \n",
    "    'TOWOD': (\n",
    "'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
    "'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant',\n",
    "'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich',\n",
    "'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote',\n",
    "'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass',\n",
    "'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}\n",
    "\n",
    "\n",
    "CLASSES = list(class_name[args.dataset])\n",
    "print(CLASSES)\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def17fc-eabd-4d69-849d-0e4d03b77915",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8bfacfb-c10a-4f22-8252-b262991fea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def viz(model, criterion, data_loader, dataset, device, output_dir='./output', allow_unknown=True):\n",
    "    mode = \"with_unknown\" if not allow_unknown else \"filtered_unknown\"\n",
    "    output_dir = os.path.join(output_dir, f\"from_dataset/{mode}/{case}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('class_error', SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "\n",
    "    num_obj = 20\n",
    "    for batch_idx, (samples, targets) in enumerate(tqdm(data_loader)):\n",
    "        if batch_idx < 4700: continue\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        top_k = len(targets[0]['boxes'])\n",
    "\n",
    "        outputs = model(samples)\n",
    "        if allow_unknown:\n",
    "            probas = outputs['pred_logits'].softmax(-1)[0, :, :].cpu()\n",
    "        else:\n",
    "            probas = outputs['pred_logits'].softmax(-1)[0, :, :-1].cpu()\n",
    "        pred_objs = outputs['pred_obj'].softmax(-1)[0, :].cpu()\n",
    "        predicted_boxes = outputs['pred_boxes'][0,].cpu()\n",
    "        scores, predicted_boxes = filter_boxes(probas, predicted_boxes)\n",
    "        labels = scores.argmax(axis=1)\n",
    "        scores = scores.max(-1).values\n",
    "\n",
    "        if allow_unknown:\n",
    "            iou_threshold = 0.4\n",
    "            unknown_class_idx = 80  # Assuming the last class is \"unknown\"\n",
    "            keep_indices = []\n",
    "\n",
    "            # Get indices of \"unknown\" predictions\n",
    "            unknown_indices = (labels == unknown_class_idx).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            # Loop over each \"unknown\" bounding box\n",
    "            for idx, (box, lbl) in enumerate(zip(predicted_boxes, labels)):\n",
    "                if lbl == unknown_class_idx:\n",
    "                    keep_unknown = True\n",
    "                    relevant_boxes = [predicted_boxes[i] for i in range(0, len(labels)) if labels[i] < 80 or i>idx]\n",
    "                    # Start comparing with boxes after the current one.\n",
    "                    for known_box in relevant_boxes:\n",
    "                        if overlap_rate(samples.tensors[0:1], box, known_box) > iou_threshold:\n",
    "                            keep_unknown = False\n",
    "                            break\n",
    "                    if keep_unknown:\n",
    "                        keep_indices.append(idx)\n",
    "                else:\n",
    "                    keep_indices.append(idx)\n",
    "            \n",
    "            # Filter using keep_indices\n",
    "            predicted_boxes = predicted_boxes[keep_indices]\n",
    "            labels = labels[keep_indices]\n",
    "            scores = scores[keep_indices]\n",
    "                    \n",
    "\n",
    "        # Save bbox, label, and probability info\n",
    "        save_bbox_info(output_dir, \n",
    "                       targets[0][\"image_id\"][0],  \n",
    "                       predicted_boxes[-top_k:].detach().cpu(), \n",
    "                       labels[-top_k:], \n",
    "                       scores[-top_k:],\n",
    "                       samples.tensors[0:1], \n",
    "                       pretrain.split('/')[-1])\n",
    "\n",
    "        # Save ground truth info\n",
    "        save_bbox_info(output_dir, \n",
    "                       targets[0][\"image_id\"][0],  \n",
    "                       targets[0]['boxes'].detach().cpu(),\n",
    "                       targets[0]['labels'], \n",
    "                       torch.ones(targets[0]['boxes'].shape[0]), \n",
    "                       samples.tensors[0:1], \n",
    "                       \"gt\")\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(10,3), dpi=200)\n",
    "\n",
    "        # Ori Picture\n",
    "        plot_ori_image(\n",
    "            samples.tensors[0:1],\n",
    "            ax[0], \n",
    "            plot_prob=False,\n",
    "        )\n",
    "        ax[0].set_title('Original Image')\n",
    "\n",
    "        plot_prediction(\n",
    "            samples.tensors[0:1], \n",
    "            scores[-top_k:], \n",
    "            predicted_boxes[-top_k:].detach().cpu(), \n",
    "            labels[-top_k:], \n",
    "            ax[1], \n",
    "            plot_prob=False,\n",
    "            dataset=dataset,\n",
    "        )\n",
    "        ax[1].set_title('Prediction (Ours)')\n",
    "\n",
    "        # GT Results\n",
    "        plot_prediction(\n",
    "            samples.tensors[0:1], \n",
    "            torch.ones(targets[0]['boxes'].shape[0]), \n",
    "            targets[0]['boxes'].detach().cpu(), \n",
    "            targets[0]['labels'],\n",
    "            ax[2], \n",
    "            plot_prob=False,\n",
    "            dataset=dataset,\n",
    "        )\n",
    "        ax[2].set_title('GT')\n",
    "\n",
    "        for i in range(3):\n",
    "            ax[i].set_aspect('equal')\n",
    "            ax[i].set_axis_off()\n",
    "        \n",
    "        #plt.savefig(os.path.join(output_dir, f\"{targets[0]['image_id'][0]}_{pretrain.split('/')[-1]}.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a29eea5-7a9f-4044-8da7-97b3af4fb047",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def viz_single_img(model, criterion, image_path, dataset, device, output_dir='./output', allow_unknown=True):\n",
    "    mode = \"with_unknown\" if not allow_unknown else \"filtered_unknown\"\n",
    "    output_dir = os.path.join(output_dir, f\"from_custom/{mode}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    use_topk = True\n",
    "    num_obj = 20\n",
    "\n",
    "    # Load the image and preprocess\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    # Normalize the image tensor\n",
    "    means = torch.tensor([0.485, 0.456, 0.406]).to(device).view(1, 3, 1, 1)\n",
    "    stds = torch.tensor([0.229, 0.224, 0.225]).to(device).view(1, 3, 1, 1)\n",
    "    image_tensor = (image_tensor - means) / stds\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(image_tensor)\n",
    "\n",
    "    if allow_unknown:\n",
    "        probas = outputs['pred_logits'].softmax(-1)[0, :, :].cpu()\n",
    "    else:\n",
    "        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1].cpu()\n",
    "    predicted_boxes = outputs['pred_boxes'][0,].cpu()\n",
    "\n",
    "    # print(probas, predicted_boxes)\n",
    "    scores, predicted_boxes = filter_boxes(probas, predicted_boxes)\n",
    "    labels = scores.argmax(axis=1)\n",
    "    scores = scores.max(-1).values  \n",
    "\n",
    "\n",
    "    iou_threshold = 0.4\n",
    "    unknown_class_idx = 80  # Assuming the last class is \"unknown\"\n",
    "    keep_indices = []\n",
    "\n",
    "    # Get indices of \"unknown\" predictions\n",
    "    unknown_indices = (labels == unknown_class_idx).nonzero(as_tuple=True)[0]\n",
    "    # print(f\"unknown indicies: {unknown_indices} - {labels}\")\n",
    "    \n",
    "    # Loop over each \"unknown\" bounding box\n",
    "    for idx, (box, lbl) in enumerate(zip(predicted_boxes, labels)):\n",
    "        if lbl == unknown_class_idx:\n",
    "            keep_unknown = True\n",
    "            for known_box in predicted_boxes[labels != unknown_class_idx]:\n",
    "                if overlap_rate(image_tensor[0:1], box, known_box) > iou_threshold:\n",
    "                    keep_unknown = False\n",
    "                    # print(\"drop\")\n",
    "                    break\n",
    "            if keep_unknown:\n",
    "                keep_indices.append(idx)\n",
    "        else:\n",
    "            keep_indices.append(idx)\n",
    "        \n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10,3), dpi=200)\n",
    "\n",
    "    # Original Image\n",
    "    plot_ori_image(image_tensor[0:1], ax[0], plot_prob=False)\n",
    "    ax[0].set_title('Original Image')\n",
    "\n",
    "    # Prediction\n",
    "    top_k = 5\n",
    "    # Ori Picture\n",
    "    plot_ori_image(\n",
    "        image_tensor[0:1],\n",
    "        ax[0], \n",
    "        plot_prob=False,\n",
    "    )\n",
    "    ax[0].set_title('Original Image', fontsize = 7)\n",
    "\n",
    "    # Pred results\n",
    "    # if not control the number of labels\n",
    "    plot_prediction(\n",
    "        image_tensor[0:1], \n",
    "        scores[-num_obj:], \n",
    "        predicted_boxes[-num_obj:].detach().cpu(), \n",
    "        labels[-num_obj:], \n",
    "        ax[1], \n",
    "        plot_prob=False,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    ax[1].set_title('Prediction (No Filter, No Top-k)', fontsize = 7)\n",
    "\n",
    "    # Use the indices to filter the predictions\n",
    "    predicted_boxes = predicted_boxes[keep_indices]\n",
    "    labels = labels[keep_indices]\n",
    "    scores = scores[keep_indices]\n",
    "\n",
    "    plot_prediction(\n",
    "        image_tensor[0:1], \n",
    "        scores[-num_obj:], \n",
    "        predicted_boxes[-num_obj:].detach().cpu(), \n",
    "        labels[-num_obj:], \n",
    "        ax[2], \n",
    "        plot_prob=False,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    ax[2].set_title('Prediction (Filtered, no Top-k)', fontsize = 7)\n",
    "\n",
    "    # if control the number of labels\n",
    "    plot_prediction(\n",
    "        image_tensor[0:1], \n",
    "        scores[-top_k:], \n",
    "        predicted_boxes[-top_k:].detach().cpu(), \n",
    "        labels[-top_k:], \n",
    "        ax[3], \n",
    "        plot_prob=False,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    ax[3].set_title('Prediction (w/ Top-k)', fontsize = 7)\n",
    "\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].set_aspect('equal')\n",
    "        ax[i].set_axis_off()\n",
    "\n",
    "    # plt.savefig(os.path.join(output_dir, f\"{os.path.basename(image_path).split('.')[0]}_{pretrain.split('/')[-1]}.jpg\"))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2771dd50-8f86-4e04-a9fd-dc0d2e652490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_rate(image, boxA, boxB):\n",
    "    h, w = image[0].permute(1, 2, 0).detach().cpu().numpy().shape[:-1]\n",
    "    boxA = rescale_bboxes(boxA, [w, h]).detach().cpu().numpy()\n",
    "    boxB = rescale_bboxes(boxB, [w, h]).detach().cpu().numpy()\n",
    "    # print(boxA, boxB)\n",
    "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # Compute the area of box A\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "\n",
    "    # Compute the overlap rate\n",
    "    rate = interArea / boxAArea\n",
    "    return rate\n",
    "\n",
    "### You can choose confidence: the default value of confidence is 0.7\n",
    "def filter_boxes(scores, boxes, confidence=0.8, apply_nms=True, iou=0.3):\n",
    "    keep = scores.max(-1).values > confidence\n",
    "    scores, boxes = scores[keep], boxes[keep]\n",
    " \n",
    "    if apply_nms:\n",
    "        top_scores, labels = scores.max(-1)\n",
    "        keep = batched_nms(boxes, top_scores, labels, iou)\n",
    "        scores, boxes = scores[keep], boxes[keep]\n",
    " \n",
    "    return scores, boxes\n",
    "\n",
    "def save_bbox_info(output_dir, image_id, bboxes, labels, scores, image, prefix):\n",
    "    \"\"\"\n",
    "    Save bounding box, label, and probability info to a text file.\n",
    "    \"\"\"\n",
    "    h, w = image[0].permute(1, 2, 0).detach().cpu().numpy().shape[:-1]  \n",
    "    bboxes = [rescale_bboxes(bboxes[i], [w, h]).cpu() for i in range(len(bboxes))]\n",
    "    with open(os.path.join(output_dir, f\"{image_id}_{prefix}.txt\"), 'w') as f:\n",
    "        for bbox, label_index, score in zip(bboxes, labels, scores):\n",
    "            label_name = CLASSES[label_index]\n",
    "            f.write(f\"BBOX: {bbox}, LABEL: {label_name} (ID: {label_index.item()}), PROB: {score.item()}\\n\")\n",
    "        f.write(f\"IMAGE_INFO: {h}, {w}\\n\")\n",
    "\n",
    "def bbox_iou(boxA, boxB):\n",
    "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # print(iou)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def plot_image(ax, img, norm):\n",
    "    if norm:\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = (img * 255)\n",
    "    img = img.astype('uint8')\n",
    "    ax.imshow(img)\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "    \n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    # print(size)\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32).to(out_bbox)\n",
    "    return b\n",
    "\n",
    "def plot_prediction(image, scores, boxes, labels, ax=None, plot_prob=True, dataset='OWOD'):    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    plot_results(image[0].permute(1, 2, 0).detach().cpu().numpy(), scores, boxes, labels, ax, plot_prob=plot_prob, dataset=dataset)\n",
    "\n",
    "def plot_ori_image(image, ax=None, plot_prob=False):    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    plot_results(image[0].permute(1, 2, 0).detach().cpu().numpy(), None, None, None, ax)\n",
    "\n",
    "def plot_results(pil_img, scores, boxes, labels, ax, plot_prob=True, norm=True, dataset='OWOD'):\n",
    "    from matplotlib import pyplot as plt\n",
    "    h, w = pil_img.shape[:-1]\n",
    "    # w, h = pil_img.shape[:-1]\n",
    "    image = plot_image(ax, pil_img, norm)\n",
    "    colors = COLORS * 100\n",
    "    if boxes is not None:\n",
    "        boxes = [rescale_bboxes(boxes[i], [w, h]).cpu() for i in range(len(boxes))]\n",
    "        for sc, cl, (xmin, ymin, xmax, ymax), c in zip(scores, labels, boxes, colors):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                       fill=False, color=c, linewidth=2))\n",
    "            # print(dataset, cl)\n",
    "            # text = f'{CLASSES[str(dataset)][cl]}: {sc:0.2f}'\n",
    "            text = f'{CLASSES[cl]}: {sc:0.2f}'\n",
    "            ax.text(xmin, ymin, text, fontsize=5, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    ax.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c297634-519a-4c1b-b0af-44ea8e8ddb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid class range: [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "DINO resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheny63/anaconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cheny63/anaconda3/envs/prob/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with exemplar_replay_selection\n",
      "Initialized from the pre-training model\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model, criterion, postprocessors, exemplar_selection = build(args)\n",
    "model.to(torch.device(args.device))\n",
    "\n",
    "print('Initialized from the pre-training model')\n",
    "checkpoint = torch.load(args.pretrain, map_location='cpu')\n",
    "state_dict = checkpoint['model']\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "args.start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2054ea90-7854-4a7b-b28a-3fe57479d561",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWDETR\n",
      "owdetr_t1_train\n",
      "owdetr_test\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 89490\n",
      "    Root location: /training_data_2/yuetian/OWOD\n",
      "    [['train'], Compose(\n",
      "    <datasets.transforms.RandomHorizontalFlip object at 0x7f99016e4850>\n",
      "    <datasets.transforms.RandomSelect object at 0x7f992269fb50>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7f99db7d45b0>\n",
      "    <datasets.transforms.Normalize object at 0x7f99016e7640>\n",
      ")\n",
      ")]\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 4952\n",
      "    Root location: /training_data_2/yuetian/OWOD\n",
      "    [['test'], Compose(\n",
      "    <datasets.transforms.RandomResize object at 0x7f99016e41f0>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7f99016e5b10>\n",
      "    <datasets.transforms.Normalize object at 0x7f99016e4220>\n",
      ")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_val = get_datasets(args)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=collate_fn, num_workers=args.num_workers,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c400ba-86bc-4d31-a7a4-f628c37df201",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz(model, criterion, data_loader_val, args.dataset, args.device, allow_unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b6bc1-af91-459b-a29b-c6cfcc159d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz_single_img(model, criterion, \"samples/cow.jpg\", args.dataset, args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ceba28-1c3a-4f79-885c-8eda2462db04",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob",
   "language": "python",
   "name": "prob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
